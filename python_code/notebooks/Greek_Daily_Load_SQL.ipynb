{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f45f50-6aea-49a7-b5cd-972c8f0fdcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload_Staging table truncated (cleared). Rows now: 0\n",
      "\n",
      "üì• Processing file: E:\\DATA\\2025-2026\\MERGE_TRADEBOOK\\MERGE_GREEK\\MergeGreek26062025.csv\n",
      "‚ö†Ô∏è Skipping MergeGreek26062025.csv ‚Äî missing columns: ['Code', 'Exchange', 'TradeDateTime']\n",
      "\n",
      "üì• Processing file: E:\\DATA\\2025-2026\\MERGE_TRADEBOOK\\MERGE_GREEK\\MergeGreek26062025_.csv\n",
      "‚úÖ Successfully appended data from MergeGreek26062025_.csv into Upload_Staging (325 rows)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# === SQL Server connection details ===\n",
    "server = 'AG-SERVER-043'\n",
    "database = '2526 GREEK'\n",
    "username = 'data05'\n",
    "password = 'sai@123'\n",
    "\n",
    "# Build SQLAlchemy engine\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password}\"\n",
    ")\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# === Folder where CSV files are stored ===\n",
    "csv_folder = r\"E:\\DATA\\2025-2026\\MERGE_TRADEBOOK\\MERGE_GREEK\"\n",
    "\n",
    "# === Automatically set today's date ===\n",
    "today = datetime.today()\n",
    "today_date_only = today.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# === Extract date from filename (format: MergeGreekDDMMYYYY.csv) ===\n",
    "def extract_date_from_filename(filename):\n",
    "    match = re.search(r'MergeGreek(\\d{8})', filename)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        try:\n",
    "            return datetime.strptime(date_str, \"%d%m%Y\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Date parse error in filename {filename}: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# === Expected columns in the CSV files ===\n",
    "expected_cols = [\n",
    "    'SourceFile', 'ExchangeTradeID', 'Symbol', 'SecurityType', 'ExpiryDate', 'StrikePrice',\n",
    "    'OptionType', 'SecurityName', 'ManagerID', 'Side', 'Quantity', 'Price', 'ClientID',\n",
    "    'MemberID', 'ExchangeOrderNo', 'ExchangeOrderStatus', 'Code', 'Exchange', 'TradeDateTime'\n",
    "]\n",
    "\n",
    "# === Find all CSVs matching today's date, excluding files with '_CLEANING_LOG' in name ===\n",
    "files_to_process = []\n",
    "for file in os.listdir(csv_folder):\n",
    "    if file.lower().endswith('.csv') and '_cleaning_log' not in file.lower():\n",
    "        file_date = extract_date_from_filename(file)\n",
    "        if file_date is not None and file_date.date() == today_date_only.date():\n",
    "            files_to_process.append(file)\n",
    "\n",
    "if files_to_process:\n",
    "    # Truncate table only if files exist to process\n",
    "    with engine.begin() as connection:\n",
    "        connection.execute(text(\"TRUNCATE TABLE Upload_Staging;\"))\n",
    "        count = connection.execute(text(\"SELECT COUNT(*) FROM Upload_Staging;\")).scalar()\n",
    "        print(f\"Upload_Staging table truncated (cleared). Rows now: {count}\")\n",
    "\n",
    "    # Loop through and process each file\n",
    "    for file in files_to_process:\n",
    "        file_path = os.path.join(csv_folder, file)\n",
    "        print(f\"\\nüì• Processing file: {file_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, quotechar='\"', dtype=str)\n",
    "            df['SourceFile'] = file\n",
    "\n",
    "            # Clean column headers\n",
    "            df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "            # Check for missing columns\n",
    "            missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"‚ö†Ô∏è Skipping {file} ‚Äî missing columns: {missing_cols}\")\n",
    "                continue\n",
    "\n",
    "            # Reorder columns to match expected order\n",
    "            df = df[expected_cols]\n",
    "\n",
    "            # Upload to SQL staging table\n",
    "            df.to_sql(name='Upload_Staging', con=engine, if_exists='append', index=False)\n",
    "            print(f\"‚úÖ Successfully appended data from {file} into Upload_Staging \"\n",
    "                  f\"({len(df)} rows)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to process {file}: {e}\")\n",
    "else:\n",
    "    print(\"üì≠ No file found for today; skipping truncation and loading.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50258ffc-2220-4fbc-8986-46e3840946e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
