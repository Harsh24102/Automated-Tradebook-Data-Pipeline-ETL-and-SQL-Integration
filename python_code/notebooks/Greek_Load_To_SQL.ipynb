{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10978af-f252-4b84-af83-3128b2977a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload_Staging table truncated (cleared).\n",
      "\n",
      "📥 Processing file: E:\\DATA\\2025-2026\\MERGE_TRADEBOOK\\MERGE_GREEK\\MergeGreek01022025.csv\n",
      "Skipping MergeGreek01022025.csv — missing columns: ['Code', 'Exchange', 'TradeDateTime']\n",
      "\n",
      "📥 Processing file: E:\\DATA\\2025-2026\\MERGE_TRADEBOOK\\MERGE_GREEK\\MergeGreek01022025_.csv\n",
      "✅ Successfully appended data from MergeGreek01022025_.csv into Upload_Staging\n",
      "\n",
      "📥 Processing file: E:\\DATA\\2025-2026\\MERGE_TRADEBOOK\\MERGE_GREEK\\MergeGreek01022025_CLEANING_LOG.csv\n",
      "Skipping MergeGreek01022025_CLEANING_LOG.csv — missing columns: ['ExchangeTradeID', 'Symbol', 'SecurityType', 'ExpiryDate', 'StrikePrice', 'OptionType', 'SecurityName', 'ManagerID', 'Side', 'Quantity', 'Price', 'ClientID', 'MemberID', 'ExchangeOrderNo', 'ExchangeOrderStatus', 'Code', 'Exchange', 'TradeDateTime']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "server = 'AG-SERVER-043'\n",
    "database = '2526 GREEK' \n",
    "username = 'data05'\n",
    "password = 'sai@123'\n",
    "\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password}\"\n",
    ")\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "csv_folder = r\"E:\\DATA\\2025-2026\\MERGE_TRADEBOOK\\MERGE_GREEK\"\n",
    "\n",
    "start_date = datetime.strptime(\"2025-02-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-02-01\", \"%Y-%m-%d\")\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    match = re.search(r'MergeGreek(\\d{8})', filename)  # removed underscore\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        try:\n",
    "            return datetime.strptime(date_str, \"%d%m%Y\")\n",
    "        except Exception as e:\n",
    "            print(f\"Date parse error in filename {filename}: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "expected_cols = [\n",
    "    'SourceFile', 'ExchangeTradeID', 'Symbol', 'SecurityType', 'ExpiryDate', 'StrikePrice',\n",
    "    'OptionType', 'SecurityName', 'ManagerID', 'Side', 'Quantity', 'Price', 'ClientID',\n",
    "    'MemberID', 'ExchangeOrderNo', 'ExchangeOrderStatus', 'Code', 'Exchange', 'TradeDateTime'\n",
    "]\n",
    "\n",
    "# First, find all files within the date range\n",
    "files_to_process = []\n",
    "for file in os.listdir(csv_folder):\n",
    "    if file.lower().endswith('.csv'):\n",
    "        file_date = extract_date_from_filename(file)\n",
    "        if file_date is not None and start_date <= file_date <= end_date:\n",
    "            files_to_process.append(file)\n",
    "\n",
    "if files_to_process:\n",
    "    # Truncate table only if files exist to process\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"TRUNCATE TABLE Upload_Staging;\"))\n",
    "        print(\"Upload_Staging table truncated (cleared).\")\n",
    "\n",
    "    # Process files\n",
    "    for file in files_to_process:\n",
    "        file_path = os.path.join(csv_folder, file)\n",
    "        print(f\"\\n📥 Processing file: {file_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, quotechar='\"', dtype=str)\n",
    "            df['SourceFile'] = file\n",
    "\n",
    "            # Clean column headers\n",
    "            df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "            # Check for missing columns\n",
    "            missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"Skipping {file} — missing columns: {missing_cols}\")\n",
    "                continue\n",
    "\n",
    "            df = df[expected_cols]\n",
    "\n",
    "            # Append data to SQL table\n",
    "            df.to_sql(name='Upload_Staging', con=engine, if_exists='append', index=False)\n",
    "            print(f\"✅ Successfully appended data from {file} into Upload_Staging\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to process {file}: {e}\")\n",
    "else:\n",
    "    print(\"No files found in the given date range; skipping truncation and loading.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c08d05-b4bc-4c4d-911b-0d24a778420b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
