{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06bd3b4-d8c2-45c0-ae92-5568c9501d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: sqlalchemy in c:\\anaconda3\\lib\\site-packages (2.0.34)\n",
      "Requirement already satisfied: pyodbc in c:\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: openpyxl in c:\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.6.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~harset-normalizer (C:\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~harset-normalizer (C:\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~harset-normalizer (C:\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas sqlalchemy pyodbc openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2858ff43-fb1a-48b5-b636-e283edf1290b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\N\\ALLOVER_EXP_June_12_NET.csv ‚Üí Table: ALLOVER_EXP_June_12_NET\n",
      "‚ùå Failed to import ALLOVER_EXP_June_12_NET.csv: Excel file format cannot be determined, you must specify an engine manually.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# ---------- SQL Server connection details ----------\n",
    "server = 'AG-SERVER-043'         # e.g. 'localhost\\SQLEXPRESS'\n",
    "database = 'Janvi'\n",
    "username = 'data03'\n",
    "password = 'sai@123'\n",
    "\n",
    "# ---------- Create the SQLAlchemy engine ----------\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# ---------- Folder path to start looking for Excel files ----------\n",
    "root_folder = r\"E:\\Back-Up Files\\Data Team\\Position & M2M\\N\"\n",
    "\n",
    "# ---------- Loop through folders and process Excel files ----------\n",
    "for dirpath, _, filenames in os.walk(root_folder):\n",
    "    for file in filenames:\n",
    "        if file.lower().endswith(('.xlsx', '.xls')):\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            table_name = os.path.splitext(file)[0].replace(' ', '_').replace('-', '_')\n",
    "\n",
    "            print(f\"\\nüì• Importing: {file_path} ‚Üí Table: {table_name}\")\n",
    "\n",
    "            try:\n",
    "                # Read the first sheet of the Excel file\n",
    "                df = pd.read_excel(file_path)\n",
    "\n",
    "                # Upload to SQL Server (replace table if exists)\n",
    "                df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "                print(f\"‚úÖ Successfully imported into table: {table_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to import {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8fa25b-91d0-4e90-b239-518a4a87bb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\Intraday_NFT_11.xlsx ‚Üí Table: Intraday_NFT_11\n",
      "‚úÖ Table 'Intraday_NFT_11' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\Intraday_NFT_15.xlsx ‚Üí Table: Intraday_NFT_15\n",
      "‚úÖ Table 'Intraday_NFT_15' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\Intraday_NFT_16.xlsx ‚Üí Table: Intraday_NFT_16\n",
      "‚úÖ Table 'Intraday_NFT_16' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\Intraday_NFT_17.xlsx ‚Üí Table: Intraday_NFT_17\n",
      "‚úÖ Table 'Intraday_NFT_17' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\Intraday_NFT_7.xlsx ‚Üí Table: Intraday_NFT_7\n",
      "‚úÖ Table 'Intraday_NFT_7' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\Intraday_NFT_8.xlsx ‚Üí Table: Intraday_NFT_8\n",
      "‚úÖ Table 'Intraday_NFT_8' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\Intraday_NFT_9.xlsx ‚Üí Table: Intraday_NFT_9\n",
      "‚úÖ Table 'Intraday_NFT_9' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\OP_NFT_11.xlsx ‚Üí Table: OP_NFT_11\n",
      "‚úÖ Table 'OP_NFT_11' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\OP_NFT_15.xlsx ‚Üí Table: OP_NFT_15\n",
      "‚úÖ Table 'OP_NFT_15' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\OP_NFT_16.xlsx ‚Üí Table: OP_NFT_16\n",
      "‚úÖ Table 'OP_NFT_16' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\OP_NFT_17.xlsx ‚Üí Table: OP_NFT_17\n",
      "‚úÖ Table 'OP_NFT_17' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\OP_NFT_7.xlsx ‚Üí Table: OP_NFT_7\n",
      "‚úÖ Table 'OP_NFT_7' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\OP_NFT_8.xlsx ‚Üí Table: OP_NFT_8\n",
      "‚úÖ Table 'OP_NFT_8' created and data inserted.\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files\\OP_NFT_9.xlsx ‚Üí Table: OP_NFT_9\n",
      "‚úÖ Table 'OP_NFT_9' created and data inserted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# ---------- SQL Server connection details ----------\n",
    "server = 'AG-SERVER-043'         # e.g., localhost\\SQLEXPRESS\n",
    "database = 'Vanshita'\n",
    "username = 'data07'\n",
    "password = 'sai@123'\n",
    "\n",
    "# ---------- SQLAlchemy connection ----------\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# ---------- Folder path to search Excel files ----------\n",
    "root_folder = r'E:\\Back-Up Files\\Data Team\\Position & M2M\\OP files'\n",
    "\n",
    "# ---------- Loop through folders and import Excel files ----------\n",
    "for dirpath, _, filenames in os.walk(root_folder):\n",
    "    for file in filenames:\n",
    "        if file.lower().endswith(('.xlsx', '.xls')):  # Only process Excel files\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            table_name = os.path.splitext(file)[0].replace(' ', '_').replace('-', '_')\n",
    "\n",
    "            print(f\"\\nüì• Importing: {file_path} ‚Üí Table: {table_name}\")\n",
    "\n",
    "            try:\n",
    "                # Read the Excel file\n",
    "                df = pd.read_excel(file_path)\n",
    "\n",
    "                # Convert 'ExpiryDate' and 'TradeDate' columns to 'YYYY-MM-DD' format (Date only, no time)\n",
    "                date_columns = ['ExpiryDate', 'TradeDate']\n",
    "                for col in date_columns:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = pd.to_datetime(df[col], errors='coerce').dt.date  # Convert to datetime, then extract date\n",
    "\n",
    "                # Insert data into SQL Server (replace table if exists)\n",
    "                df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "                print(f\"‚úÖ Table '{table_name}' created and data inserted.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to import {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0de2c-44a2-4562-8b63-973270469341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076d362-86d0-43b8-b176-f72868faf5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8488d76e-fc79-40dc-95ee-abeb90f36184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# ---------- SQL Server connection details ----------\n",
    "server = 'AG-SERVER-043'         # e.g., localhost\\SQLEXPRESS\n",
    "database = 'janvi'\n",
    "username = 'data05'\n",
    "password = 'sai@123'\n",
    "\n",
    "# ---------- SQLAlchemy connection ----------\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# ---------- Folder path to search Excel files ----------\n",
    "root_folder = r'E:\\Back-Up Files\\Data Team\\Position & M2M\\N'\n",
    "\n",
    "# ---------- Loop through folders and import Excel files ----------\n",
    "for dirpath, _, filenames in os.walk(root_folder):\n",
    "    for file in filenames:\n",
    "        if file.lower().endswith(('.xlsx', '.xls')):  # Only process Excel files\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            table_name = os.path.splitext(file)[0].replace(' ', '_').replace('-', '_')\n",
    "\n",
    "            print(f\"\\nüì• Importing: {file_path} ‚Üí Table: {table_name}\")\n",
    "\n",
    "            try:\n",
    "                # Read the Excel file\n",
    "                df = pd.read_excel(file_path)\n",
    "\n",
    "                # Convert 'ExpiryDate' and 'TradeDate' columns to 'YYYY-MM-DD' format (Date only, no time)\n",
    "                date_columns = ['ExpiryDate', 'TradeDate']\n",
    "                for col in date_columns:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = pd.to_datetime(df[col], errors='coerce').dt.date  # Convert to datetime, then extract date\n",
    "\n",
    "                # Insert data into SQL Server (replace table if exists)\n",
    "                df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "                print(f\"‚úÖ Table '{table_name}' created and data inserted.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to import {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02bec3a-4963-4043-ba4c-6e94818ab059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bc0aa-eacb-4fe4-9608-f5e4e757203b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f8620-ce48-4a36-8acc-2144421caa3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e8678-07e5-420a-9aad-e197d8124d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e77b20-5e97-43ca-b88d-a2df060ccaba",
   "metadata": {},
   "source": [
    "### Bhavcopy import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b18c87-4c3a-4992-b4d4-cadfc6c8d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\BSE - NSE\\BSE + NSE\\BHAVCOPY\\MERGED\\BHAVCOPY2025_06_16.csv ‚Üí Table: BHAVCOPY2025_06_16\n",
      "‚úÖ Successfully imported into table: BHAVCOPY2025_06_16\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\BSE - NSE\\BSE + NSE\\BHAVCOPY\\MERGED\\G_T_Bhavcopy_FO_160625.CSV ‚Üí Table: G_T_Bhavcopy_FO_160625\n",
      "‚úÖ Successfully imported into table: G_T_Bhavcopy_FO_160625\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import urllib\n",
    "\n",
    "# ---------- SQL Server connection details ----------\n",
    "server = 'AG-SERVER-043'\n",
    "database = 'Janvi'\n",
    "username = 'data03'\n",
    "password = 'sai@123'\n",
    "\n",
    "# ---------- Create the SQLAlchemy engine ----------\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# ---------- Folder path to start looking for CSV files ----------\n",
    "root_folder = r\"E:\\Back-Up Files\\Data Team\\BSE - NSE\\BSE + NSE\\BHAVCOPY\\MERGED\"\n",
    "\n",
    "# ---------- Create an inspector to check existing tables ----------\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# ---------- Loop through folders and process CSV files ----------\n",
    "for dirpath, _, filenames in os.walk(root_folder):\n",
    "    for file in filenames:\n",
    "        if file.lower().endswith('.csv'):\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            table_name = os.path.splitext(file)[0].replace(' ', '_').replace('-', '_')\n",
    "\n",
    "            # Check if table already exists\n",
    "            if table_name in inspector.get_table_names():\n",
    "                print(f\"‚è≠Ô∏è Skipping {file} ‚Üí Table '{table_name}' already exists.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nüì• Importing: {file_path} ‚Üí Table: {table_name}\")\n",
    "\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Upload to SQL Server (fail if table already exists)\n",
    "                df.to_sql(name=table_name, con=engine, if_exists='fail', index=False)\n",
    "                print(f\"‚úÖ Successfully imported into table: {table_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to import {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56f844-1fdd-47d3-b55e-617ac24e53bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b3478-3a4b-4e6c-99e8-020c4f1b813e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687edc6e-d78d-42d5-810b-14bb7c3cf4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\CLUB\\CLUB__EXPOPT.csv ‚Üí Table: CLUB__EXPOPT\n",
      "‚úÖ Table 'CLUB__EXPOPT' created and data inserted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# ---------- SQL Server connection details ----------\n",
    "server = 'AG-SERVER-043'         # e.g., localhost\\SQLEXPRESS\n",
    "database = 'BI Report'\n",
    "username = 'data03'\n",
    "password = 'sai@123'\n",
    "\n",
    "# ---------- SQLAlchemy connection ----------\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# ---------- Folder path to search Excel files ----------\n",
    "root_folder = r'E:\\Back-Up Files\\Data Team\\Position & M2M\\CLUB'\n",
    "\n",
    "# ---------- Loop through folders and import Excel files ----------\n",
    "for dirpath, _, filenames in os.walk(root_folder):\n",
    "    for file in filenames:\n",
    "        if file.lower().endswith(('.csv')):  # Only process Excel files\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            table_name = os.path.splitext(file)[0].replace(' ', '_').replace('-', '_')\n",
    "\n",
    "            print(f\"\\nüì• Importing: {file_path} ‚Üí Table: {table_name}\")\n",
    "\n",
    "            try:\n",
    "                # Read the Excel file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Convert 'ExpiryDate' and 'TradeDate' columns to 'YYYY-MM-DD' format (Date only, no time)\n",
    "                date_columns = ['ExpiryDate', 'TradeDate']\n",
    "                for col in date_columns:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = pd.to_datetime(df[col], errors='coerce').dt.date  # Convert to datetime, then extract date\n",
    "\n",
    "                # Insert data into SQL Server (replace table if exists)\n",
    "                df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "                print(f\"‚úÖ Table '{table_name}' created and data inserted.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to import {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88011d2-bce8-4679-85d1-e862118cd334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\N\\ALLOVER_EXP_June_12_NET.csv ‚Üí Table: ALLOVER_EXP_June_12_NET\n",
      "‚úÖ Successfully imported into table: ALLOVER_EXP_June_12_NET\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# ---------- SQL Server connection details ----------\n",
    "server = 'AG-SERVER-043'\n",
    "database = 'Janvi'\n",
    "username = 'data03'\n",
    "password = 'sai@123'\n",
    "\n",
    "# ---------- Create the SQLAlchemy engine ----------\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# ---------- Folder path to start looking for CSV files ----------\n",
    "root_folder = r\"E:\\Back-Up Files\\Data Team\\Position & M2M\\N\"\n",
    "\n",
    "# ---------- Loop through folders and process CSV files ----------\n",
    "for dirpath, _, filenames in os.walk(root_folder):\n",
    "    for file in filenames:\n",
    "        if file.lower().endswith('.csv'):\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            table_name = os.path.splitext(file)[0].replace(' ', '_').replace('-', '_')\n",
    "\n",
    "            print(f\"\\nüì• Importing: {file_path} ‚Üí Table: {table_name}\")\n",
    "\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Upload to SQL Server (replace table if it exists)\n",
    "                df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "                print(f\"‚úÖ Successfully imported into table: {table_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to import {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119832ef-589f-4faa-b9c0-7437b03ab709",
   "metadata": {},
   "source": [
    "### csv file import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72920cae-402b-4b14-b315-68596c5a6e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\N\\MergedTrade20250613.csv ‚Üí Table: MergedTrade20250613\n",
      "‚úÖ Successfully imported into table: MergedTrade20250613\n",
      "\n",
      "üì• Importing: E:\\Back-Up Files\\Data Team\\Position & M2M\\N\\MergedTrade20250616.csv ‚Üí Table: MergedTrade20250616\n",
      "‚úÖ Successfully imported into table: MergedTrade20250616\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# ---------- SQL Server connection details ----------\n",
    "server = 'AG-SERVER-043'\n",
    "database = 'harsh_data'\n",
    "username = 'data05'\n",
    "password = 'sai@123'\n",
    "\n",
    "# ---------- Create the SQLAlchemy engine ----------\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"UID={username};PWD={password}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# ---------- Folder path to start looking for CSV files ----------\n",
    "root_folder = r\"E:\\Back-Up Files\\Data Team\\Position & M2M\\N\"\n",
    "\n",
    "# ---------- Loop through folders and process CSV files ----------\n",
    "for dirpath, _, filenames in os.walk(root_folder):\n",
    "    for file in filenames:\n",
    "        if file.lower().endswith('.csv'):\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            table_name = os.path.splitext(file)[0].replace(' ', '_').replace('-', '_')\n",
    "\n",
    "            print(f\"\\nüì• Importing: {file_path} ‚Üí Table: {table_name}\")\n",
    "\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Upload to SQL Server (replace table if it exists)\n",
    "                df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "                print(f\"‚úÖ Successfully imported into table: {table_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to import {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f96f2-e184-486c-bed1-a01fc331e90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
